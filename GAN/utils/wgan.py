import torch
import torch.autograd as autograd


def compute_gradient_penalty(critic, target_samples, fake_samples, device):
    """Calculates the gradient penalty loss for Wasserstein GAN with gradient penalty (WGAN-GP).

    The gradient penalty encourages the gradients of the discriminator's scores wrt the
    interpolated samples to have a norm of 1.
    The function randomly interpolates the real and fake samples using epsilon-weighted average.
    It then computes the critic's scores for the interpolated samples.
    The gradients of the scores with respect to the interpolated samples are calculated using autograd.grad().
    The gradients are reshaped and the gradient penalty is computed as the mean squared norm of the gradients.
    The function returns the computed gradient penalty loss.
    Args:
        critic (torch.nn.Module): The critic network.
        target_samples (torch.Tensor): The real samples from the data distribution.
            Shape: (batch_size, C, W, H)
        fake_samples (torch.Tensor): The fake samples generated by the generator.
            Shape: (batch_size, C, W, H)
        device (torch.device): The device on which the computations will be performed.
    Returns:
        torch.Tensor: The computed gradient penalty loss.
    """
    batch_size, W, H = target_samples.shape
    # TODO: check the theory for the W-distance and your numbers, this epsilon seems way too
    #  high, like it would skew the results a lot
    epsilon = torch.rand(batch_size, 1, 1).repeat(1, W, H).to(device)
    interpolated_images = (epsilon * target_samples + ((1 - epsilon) * fake_samples))
    interpolated_scores = critic(interpolated_images)

    # Get gradient w.r.t. interpolates
    gradients = autograd.grad(
        inputs=interpolated_images,
        outputs=interpolated_scores,
        grad_outputs=torch.ones_like(interpolated_scores),
        create_graph=True,
        retain_graph=True,
    )[0]
    gradients = gradients.view(gradients.shape[0], -1)
    gradient_penalty = torch.mean((1. - torch.sqrt(1e-8+torch.sum(gradients**2, dim=1)))**2)
    return gradient_penalty